{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "810d9f10-ec19-4b09-8f90-e983e460b319",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://laelgelcpublic.s3.sa-east-1.amazonaws.com/lael_50_years_narrow_white.png.no_years.400px_96dpi.png\" width=\"300\" alt=\"LAEL 50 years logo\">\n",
    "<h3>APPLIED LINGUISTICS GRADUATE PROGRAMME (LAEL)</h3>\n",
    "</center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c2c96-2fc3-4a1a-995b-c388036a2a15",
   "metadata": {},
   "source": [
    "# Corpus Linguistics - Study 1 - Mariana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3ad8a7-2d34-476b-be69-5d896b27d3ed",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1209cf-0515-45b3-8856-275ac692943e",
   "metadata": {},
   "source": [
    "Make sure the prerequisites in [CL_LMDA_prerequisites](https://github.com/laelgelc/laelgelc/blob/main/CL_LMDA_prerequisites.ipynb) are satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28897518-a858-4dc5-b510-49c9131da966",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f58941-1578-42c0-8933-d8ebef369245",
   "metadata": {},
   "source": [
    "Please download the following dataset (Right-click on the link and choose `Save link as` to download the corresponding file):\n",
    "- [mari201901.jsonl](https://laelgelcawsemrmariana.s3.sa-east-1.amazonaws.com/mari201901.jsonl)\n",
    "\n",
    "Please refer to [Tweet object](https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c2af7-9fc1-4f51-a4f5-2ed915b93039",
   "metadata": {},
   "source": [
    "## Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6689499-d19d-4b0c-befd-f88926b7105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import demoji\n",
    "import re\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a79a002-c182-4021-9b77-99ad470582af",
   "metadata": {},
   "source": [
    "## Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e37aae-8cee-42ca-91ff-10f882fc45c6",
   "metadata": {},
   "source": [
    "### Importing the tweet raw data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a762708-07e4-472d-b3e9-bb0cea14438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_raw_data = pd.read_json('mari201901.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623ddacd-5a20-4d89-a48e-98849a0644e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>filter_level</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>lang</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>...</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>geo</th>\n",
       "      <th>place</th>\n",
       "      <th>withheld_in_countries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-15 14:07:11+00:00</td>\n",
       "      <td>{'hashtags': [{'indices': [67, 77], 'text': 'V...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>1085176574671048705</td>\n",
       "      <td>1085176574671048704</td>\n",
       "      <td>False</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-15 14:07:11+00:00</td>\n",
       "      <td>{'hashtags': [{'indices': [67, 77], 'text': 'V...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>1085176574671048705</td>\n",
       "      <td>1085176574671048704</td>\n",
       "      <td>False</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-13 15:05:36+00:00</td>\n",
       "      <td>{'hashtags': [{'indices': [38, 48], 'text': 'V...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>1084466499962707968</td>\n",
       "      <td>1084466499962707968</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-13 15:05:36+00:00</td>\n",
       "      <td>{'hashtags': [{'indices': [38, 48], 'text': 'V...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>1084466499962707968</td>\n",
       "      <td>1084466499962707968</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-13 13:36:24+00:00</td>\n",
       "      <td>{'hashtags': [{'indices': [28, 38], 'text': 'V...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>1084444052030976002</td>\n",
       "      <td>1084444052030976000</td>\n",
       "      <td>True</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at  \\\n",
       "0 2019-01-15 14:07:11+00:00   \n",
       "1 2019-01-15 14:07:11+00:00   \n",
       "2 2019-01-13 15:05:36+00:00   \n",
       "3 2019-01-13 15:05:36+00:00   \n",
       "4 2019-01-13 13:36:24+00:00   \n",
       "\n",
       "                                            entities  favorite_count  \\\n",
       "0  {'hashtags': [{'indices': [67, 77], 'text': 'V...               0   \n",
       "1  {'hashtags': [{'indices': [67, 77], 'text': 'V...               0   \n",
       "2  {'hashtags': [{'indices': [38, 48], 'text': 'V...               0   \n",
       "3  {'hashtags': [{'indices': [38, 48], 'text': 'V...               0   \n",
       "4  {'hashtags': [{'indices': [28, 38], 'text': 'V...               0   \n",
       "\n",
       "   favorited filter_level                   id               id_str  \\\n",
       "0      False          low  1085176574671048705  1085176574671048704   \n",
       "1      False          low  1085176574671048705  1085176574671048704   \n",
       "2      False          low  1084466499962707968  1084466499962707968   \n",
       "3      False          low  1084466499962707968  1084466499962707968   \n",
       "4      False          low  1084444052030976002  1084444052030976000   \n",
       "\n",
       "   is_quote_status lang  quote_count  ...  possibly_sensitive  \\\n",
       "0            False   es            0  ...                 NaN   \n",
       "1            False   es            0  ...                 NaN   \n",
       "2             True   en            0  ...                 NaN   \n",
       "3             True   en            0  ...                 NaN   \n",
       "4             True   es            0  ...                 NaN   \n",
       "\n",
       "   in_reply_to_screen_name  in_reply_to_user_id in_reply_to_user_id_str  \\\n",
       "0                      NaN                  NaN                     NaN   \n",
       "1                      NaN                  NaN                     NaN   \n",
       "2                      NaN                  NaN                     NaN   \n",
       "3                      NaN                  NaN                     NaN   \n",
       "4                      NaN                  NaN                     NaN   \n",
       "\n",
       "  in_reply_to_status_id in_reply_to_status_id_str coordinates  geo place  \\\n",
       "0                   NaN                       NaN         NaN  NaN   NaN   \n",
       "1                   NaN                       NaN         NaN  NaN   NaN   \n",
       "2                   NaN                       NaN         NaN  NaN   NaN   \n",
       "3                   NaN                       NaN         NaN  NaN   NaN   \n",
       "4                   NaN                       NaN         NaN  NaN   NaN   \n",
       "\n",
       "  withheld_in_countries  \n",
       "0                   NaN  \n",
       "1                   NaN  \n",
       "2                   NaN  \n",
       "3                   NaN  \n",
       "4                   NaN  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_raw_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ef5290-f5bf-44ae-8f40-5d1274b8318a",
   "metadata": {},
   "source": [
    "### Checking if data types are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6795c314-cc1f-4acd-86a1-a4eada8a9bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at                   datetime64[ns, UTC]\n",
       "entities                                  object\n",
       "favorite_count                             int64\n",
       "favorited                                   bool\n",
       "filter_level                              object\n",
       "id                                         int64\n",
       "id_str                                    object\n",
       "is_quote_status                             bool\n",
       "lang                                      object\n",
       "quote_count                                int64\n",
       "reply_count                                int64\n",
       "retweet_count                              int64\n",
       "retweeted                                   bool\n",
       "retweeted_status                          object\n",
       "source                                    object\n",
       "text                                      object\n",
       "timestamp_ms                      datetime64[ns]\n",
       "truncated                                   bool\n",
       "user                                      object\n",
       "quoted_status                             object\n",
       "quoted_status_id                         float64\n",
       "quoted_status_id_str                     float64\n",
       "quoted_status_permalink                   object\n",
       "extended_tweet                            object\n",
       "display_text_range                        object\n",
       "extended_entities                         object\n",
       "possibly_sensitive                       float64\n",
       "in_reply_to_screen_name                   object\n",
       "in_reply_to_user_id                      float64\n",
       "in_reply_to_user_id_str                  float64\n",
       "in_reply_to_status_id                    float64\n",
       "in_reply_to_status_id_str                float64\n",
       "coordinates                               object\n",
       "geo                                       object\n",
       "place                                     object\n",
       "withheld_in_countries                     object\n",
       "username                                  object\n",
       "author_id                                 object\n",
       "tweet_url                                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_raw_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f52ac-c869-45c0-885e-65ec0b04d4e9",
   "metadata": {},
   "source": [
    "#### Converting `id_str` column's data type to `str`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbd7544b-6bce-4276-bde0-457e6c378d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_raw_data['id_str'] = df_tweets_raw_data['id_str'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ff2a3f-1c43-4a31-b876-ff6de54714e6",
   "metadata": {},
   "source": [
    "### Listing the values of the parameter `lang`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3db8bc9e-271b-4702-851e-6d23aa3c5d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['es', 'en', 'und', 'pt', 'ca', 'fr', 'eu', 'it', 'de', 'ar', 'ht',\n",
       "       'zh', 'fa', 'tr', 'sv', 'cy', 'ur', 'ro', 'in', 'uk', 'el', 'hi',\n",
       "       'nl', 'pl', 'ru', 'cs', 'tl', 'fi', 'no', 'lt', 'ja', 'et', 'sr',\n",
       "       'hu', 'da'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_raw_data['lang'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11784db-465b-46e8-bb33-d06bcb1cfb37",
   "metadata": {},
   "source": [
    "### Keeping only the tweets in Portuguese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6de7913-03e4-4225-8f1a-1588e28b7f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_raw_data = df_tweets_raw_data[df_tweets_raw_data['lang'] == 'pt'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406c9ba9-73ad-42a3-bfdb-f64cc36c96a9",
   "metadata": {},
   "source": [
    "### Extracting the column `username`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0079390d-6438-4d72-bf6e-0cf2278898ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the nested JSON 'user' attribute\n",
    "df_tweets_raw_data_flattened_user = pd.json_normalize(df_tweets_raw_data['user'])\n",
    "\n",
    "# Extract the 'screen_name' attribute\n",
    "username = df_tweets_raw_data_flattened_user['screen_name']\n",
    "\n",
    "# Create a new column 'username'\n",
    "df_tweets_raw_data['username'] = username"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1224c888-1bd9-4c03-be3d-afb25f333bbb",
   "metadata": {},
   "source": [
    "### Extracting the column `author_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b3b0049-4e58-4a80-96c6-199b851d0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'id_str' attribute\n",
    "author_id = df_tweets_raw_data_flattened_user['id_str']\n",
    "\n",
    "# Create a new column 'username'\n",
    "df_tweets_raw_data['author_id'] = author_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e98ef-4398-4d56-8af8-35eb031d0155",
   "metadata": {},
   "source": [
    "### Extracting the column `tweet_url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04d1211f-1cca-4e9c-8041-0d6579ab32a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the tweet URL using the tweet ID and user's screen name\n",
    "df_tweets_raw_data['tweet_url'] = (\n",
    "    'https://twitter.com/' + \n",
    "    df_tweets_raw_data['username'] + \n",
    "    '/status/' + \n",
    "    df_tweets_raw_data['id_str']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376d20eb-453c-4818-9906-69bbffe3a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ceb9ed2-01f5-4b06-87c8-a84a1fdaec44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1084480777365139456\n",
       "1      1084480777365139456\n",
       "2      1084480357959901184\n",
       "3      1084480357959901184\n",
       "4      1084479661709631488\n",
       "              ...         \n",
       "677    1088340342347313152\n",
       "678    1088342930245464064\n",
       "679    1087630380897832960\n",
       "680    1088353160173928448\n",
       "681    1087963693873287168\n",
       "Name: id_str, Length: 682, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_raw_data['id_str']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4277d732-f65b-4e0c-868d-16d2d38ef04b",
   "metadata": {},
   "source": [
    "### Inspecting the dataset and eliminating malformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30643060-8eaa-47e0-9a52-95f6fbd75515",
   "metadata": {},
   "source": [
    "#### Identifying rows that are empty in column `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de7bb2-56d3-41cb-9294-d19dbeca836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_tweets_raw_data['text'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e9cf4-0f39-4cdc-84f0-84d8446eea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_raw_data[df_tweets_raw_data['text'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bbc61e-77c5-4339-99ed-e8aea3a1ec42",
   "metadata": {},
   "source": [
    "#### Dropping the rows that are empty in the column `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62e8de-5e85-4474-9bf4-4c9342c2640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows whose column 'text' is NaN\n",
    "df_tweets_raw_data = df_tweets_raw_data.dropna(subset=['text'])\n",
    "\n",
    "# Reset the index\n",
    "df_tweets_raw_data = df_tweets_raw_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d9a76-464b-41ca-842a-921beae1c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_tweets_raw_data['text'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd59c55-0719-4485-ad8c-2f76fc5b21ee",
   "metadata": {},
   "source": [
    "#### Removing specific Unicode characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017cc0ca-8a41-4c78-a341-3622a22244a5",
   "metadata": {},
   "source": [
    "The dataset may need to be cleaned of invisible Unicode characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93248ba8-48c5-466a-ac42-f51705ecdf91",
   "metadata": {},
   "source": [
    "##### Detecting `U+2066` and `U+2069` characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7838b4-b5dd-4a45-ae3a-e4f3f8408a2b",
   "metadata": {},
   "source": [
    "- [U+2066](https://www.compart.com/en/unicode/U+2066)\n",
    "- [U+2069](https://www.compart.com/en/unicode/U+2069)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1feac56-081a-4197-85ec-363a958848fb",
   "metadata": {},
   "source": [
    "Please refer to:\n",
    "- [Python RegEx](https://www.w3schools.com/python/python_regex.asp)\n",
    "- [regex101](https://regex101.com/)\n",
    "- [RegExr](https://regexr.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569db1bb-d6ee-4eca-8383-5dfb522f35aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to detect specific Unicode characters\n",
    "def extract_unicode_characters(df, column_name):\n",
    "    unicode_chars = Counter()  # Initialize a Counter to store Unicode character counts\n",
    "\n",
    "    for value in df[column_name]:\n",
    "        if isinstance(value, str):\n",
    "            # Use RegEx to find non-ASCII characters (Unicode)\n",
    "#            non_ascii_chars = re.findall(r'[^\\x00-\\x7F]+', value)\n",
    "            # Use RegEx to find specific Unicode characters - adjust the expression accordingly\n",
    "            specific_unicode_chars = re.findall(r'[\\u2066\\u2069]', value)\n",
    "            unicode_chars.update(specific_unicode_chars)\n",
    "\n",
    "    return unicode_chars\n",
    "\n",
    "# Inspect the dataframe for specific Unicode characters\n",
    "unicode_counts = extract_unicode_characters(df_tweets_raw_data, 'text')\n",
    "\n",
    "# Print the results\n",
    "for char, count in unicode_counts.items():\n",
    "    print(f'Character {char}: Count = {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c3a43b-31d1-47d2-8bc3-c701c5450b3a",
   "metadata": {},
   "source": [
    "##### Removing `U+2066` and `U+2069` characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795d7929-bb59-4417-87e5-1448a3662264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to remove specific Unicode characters\n",
    "def remove_specific_unicode(input_line):\n",
    "    # Using RegEx to replace specific Unicode characters - adjust the expression accordingly\n",
    "    cleaned_line = re.sub(r'[\\u2066\\u2069]', '', input_line)\n",
    "    return cleaned_line\n",
    "\n",
    "# Removing specific Unicode characters\n",
    "df_tweets_raw_data['text'] = df_tweets_raw_data['text'].apply(remove_specific_unicode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c87ca3-d26d-450c-8a8e-669a3bc51afb",
   "metadata": {},
   "source": [
    "### Dropping duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eee4c7-06e1-4ac6-9fd6-0080e0aa3403",
   "metadata": {},
   "source": [
    "#### Retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a7b99f-669d-486a-a247-820c7066c45f",
   "metadata": {},
   "source": [
    "Retweets bear the RegEx pattern `/\\bRT @/gm` or `/\\brt @/gm` at the beginning of the column `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7bb1f9-796e-45ef-8ad6-e4adfe11c1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a boolean mask for filtering - it is preceded by '~' to invert the selection\n",
    "mask = ~df_tweets_raw_data['text'].str.contains(r'\\bRT @|\\brt @', regex=True)\n",
    "\n",
    "# Applying the mask to overwrite the raw data dataframe with non retweeted tweets\n",
    "df_tweets_raw_data = df_tweets_raw_data[mask]\n",
    "df_tweets_raw_data = df_tweets_raw_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a00a1-f622-43c8-a804-0dce95c47262",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_raw_data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b71129d-96b8-48aa-be25-f7ff6f1d58f0",
   "metadata": {},
   "source": [
    "#### Duplicate tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e176ee8-41a6-457a-8df4-9b848e0f86a0",
   "metadata": {},
   "source": [
    "The dataset was build in a way that if a certain tweet had more than one photo, one copy of the tweet was included per unique photo. Since we are concerned with analysing just the text, those duplicates should be removed. Tweets that bear the same 'tweet_url' are duplicates - we are going to keep only the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f236ebb-5370-4a45-8a79-9809457183df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows except the first occurrence based on 'text'\n",
    "df_tweets_raw_data.drop_duplicates(subset='id_str', keep='first', inplace=True)\n",
    "df_tweets_raw_data = df_tweets_raw_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6679e8c-c5f0-4d9a-8981-aa47e2542f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>filter_level</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>lang</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>...</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>geo</th>\n",
       "      <th>place</th>\n",
       "      <th>withheld_in_countries</th>\n",
       "      <th>username</th>\n",
       "      <th>author_id</th>\n",
       "      <th>tweet_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-13 16:02:20+00:00</td>\n",
       "      <td>{'hashtags': [{'indices': [17, 25], 'text': 'U...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>1084480777365139458</td>\n",
       "      <td>1084480777365139456</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TaconThiago</td>\n",
       "      <td>1057374436624609281</td>\n",
       "      <td>https://twitter.com/TaconThiago/status/1084480...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-13 16:00:40+00:00</td>\n",
       "      <td>{'hashtags': [{'indices': [17, 25], 'text': 'U...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>1084480357959901186</td>\n",
       "      <td>1084480357959901184</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brunobr18373270</td>\n",
       "      <td>1076139993599541248</td>\n",
       "      <td>https://twitter.com/Brunobr18373270/status/108...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-13 15:57:54+00:00</td>\n",
       "      <td>{'hashtags': [{'indices': [17, 25], 'text': 'U...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>1084479661709631494</td>\n",
       "      <td>1084479661709631488</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pedrodon17</td>\n",
       "      <td>1052964117181583361</td>\n",
       "      <td>https://twitter.com/Pedrodon17/status/10844796...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-13 15:59:09+00:00</td>\n",
       "      <td>{'hashtags': [{'indices': [17, 25], 'text': 'U...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>1084479976244690946</td>\n",
       "      <td>1084479976244690944</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RanieriXBarbosa</td>\n",
       "      <td>936990437100879873</td>\n",
       "      <td>https://twitter.com/RanieriXBarbosa/status/108...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-13 16:09:20+00:00</td>\n",
       "      <td>{'hashtags': [{'indices': [17, 25], 'text': 'U...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>1084482538993782784</td>\n",
       "      <td>1084482538993782784</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Borges2510</td>\n",
       "      <td>1712396508</td>\n",
       "      <td>https://twitter.com/Borges2510/status/10844825...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>2019-01-24 07:38:52+00:00</td>\n",
       "      <td>{'hashtags': [{'indices': [35, 41], 'text': 't...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>1088340342347313152</td>\n",
       "      <td>1088340342347313152</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tioito1</td>\n",
       "      <td>982506039055716352</td>\n",
       "      <td>https://twitter.com/Tioito1/status/10883403423...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>2019-01-24 07:49:09+00:00</td>\n",
       "      <td>{'hashtags': [{'indices': [19, 30], 'text': 'C...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>1088342930245464065</td>\n",
       "      <td>1088342930245464064</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jonh__fox</td>\n",
       "      <td>896205547</td>\n",
       "      <td>https://twitter.com/jonh__fox/status/108834293...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>2019-01-22 08:37:44+00:00</td>\n",
       "      <td>{'hashtags': [{'indices': [76, 86], 'text': 'V...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>1087630380897832960</td>\n",
       "      <td>1087630380897832960</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ppdegalicia</td>\n",
       "      <td>13494262</td>\n",
       "      <td>https://twitter.com/ppdegalicia/status/1087630...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>2019-01-24 08:29:48+00:00</td>\n",
       "      <td>{'hashtags': [{'indices': [24, 34], 'text': 'V...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>1088353160173928448</td>\n",
       "      <td>1088353160173928448</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mavitrejo</td>\n",
       "      <td>980262593515356160</td>\n",
       "      <td>https://twitter.com/mavitrejo/status/108835316...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>2019-01-23 06:42:12+00:00</td>\n",
       "      <td>{'hashtags': [{'indices': [14, 37], 'text': '2...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>1087963693873287175</td>\n",
       "      <td>1087963693873287168</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oliveira27M</td>\n",
       "      <td>2845897565</td>\n",
       "      <td>https://twitter.com/Oliveira27M/status/1087963...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>619 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   created_at  \\\n",
       "0   2019-01-13 16:02:20+00:00   \n",
       "1   2019-01-13 16:00:40+00:00   \n",
       "2   2019-01-13 15:57:54+00:00   \n",
       "3   2019-01-13 15:59:09+00:00   \n",
       "4   2019-01-13 16:09:20+00:00   \n",
       "..                        ...   \n",
       "614 2019-01-24 07:38:52+00:00   \n",
       "615 2019-01-24 07:49:09+00:00   \n",
       "616 2019-01-22 08:37:44+00:00   \n",
       "617 2019-01-24 08:29:48+00:00   \n",
       "618 2019-01-23 06:42:12+00:00   \n",
       "\n",
       "                                              entities  favorite_count  \\\n",
       "0    {'hashtags': [{'indices': [17, 25], 'text': 'U...               0   \n",
       "1    {'hashtags': [{'indices': [17, 25], 'text': 'U...               0   \n",
       "2    {'hashtags': [{'indices': [17, 25], 'text': 'U...               0   \n",
       "3    {'hashtags': [{'indices': [17, 25], 'text': 'U...               0   \n",
       "4    {'hashtags': [{'indices': [17, 25], 'text': 'U...               0   \n",
       "..                                                 ...             ...   \n",
       "614  {'hashtags': [{'indices': [35, 41], 'text': 't...               0   \n",
       "615  {'hashtags': [{'indices': [19, 30], 'text': 'C...               0   \n",
       "616  {'hashtags': [{'indices': [76, 86], 'text': 'V...               0   \n",
       "617  {'hashtags': [{'indices': [24, 34], 'text': 'V...               0   \n",
       "618  {'hashtags': [{'indices': [14, 37], 'text': '2...               0   \n",
       "\n",
       "     favorited filter_level                   id               id_str  \\\n",
       "0        False          low  1084480777365139458  1084480777365139456   \n",
       "1        False          low  1084480357959901186  1084480357959901184   \n",
       "2        False          low  1084479661709631494  1084479661709631488   \n",
       "3        False          low  1084479976244690946  1084479976244690944   \n",
       "4        False          low  1084482538993782784  1084482538993782784   \n",
       "..         ...          ...                  ...                  ...   \n",
       "614      False          low  1088340342347313152  1088340342347313152   \n",
       "615      False          low  1088342930245464065  1088342930245464064   \n",
       "616      False          low  1087630380897832960  1087630380897832960   \n",
       "617      False          low  1088353160173928448  1088353160173928448   \n",
       "618      False          low  1087963693873287175  1087963693873287168   \n",
       "\n",
       "     is_quote_status lang  quote_count  ...  in_reply_to_user_id_str  \\\n",
       "0              False   pt            0  ...                      NaN   \n",
       "1              False   pt            0  ...                      NaN   \n",
       "2              False   pt            0  ...                      NaN   \n",
       "3              False   pt            0  ...                      NaN   \n",
       "4              False   pt            0  ...                      NaN   \n",
       "..               ...  ...          ...  ...                      ...   \n",
       "614            False   pt            0  ...                      NaN   \n",
       "615            False   pt            0  ...                      NaN   \n",
       "616            False   pt            0  ...                      NaN   \n",
       "617            False   pt            0  ...                      NaN   \n",
       "618            False   pt            0  ...                      NaN   \n",
       "\n",
       "     in_reply_to_status_id  in_reply_to_status_id_str coordinates  geo place  \\\n",
       "0                      NaN                        NaN         NaN  NaN   NaN   \n",
       "1                      NaN                        NaN         NaN  NaN   NaN   \n",
       "2                      NaN                        NaN         NaN  NaN   NaN   \n",
       "3                      NaN                        NaN         NaN  NaN   NaN   \n",
       "4                      NaN                        NaN         NaN  NaN   NaN   \n",
       "..                     ...                        ...         ...  ...   ...   \n",
       "614                    NaN                        NaN         NaN  NaN   NaN   \n",
       "615                    NaN                        NaN         NaN  NaN   NaN   \n",
       "616                    NaN                        NaN         NaN  NaN   NaN   \n",
       "617                    NaN                        NaN         NaN  NaN   NaN   \n",
       "618                    NaN                        NaN         NaN  NaN   NaN   \n",
       "\n",
       "    withheld_in_countries         username            author_id  \\\n",
       "0                     NaN      TaconThiago  1057374436624609281   \n",
       "1                     NaN  Brunobr18373270  1076139993599541248   \n",
       "2                     NaN       Pedrodon17  1052964117181583361   \n",
       "3                     NaN  RanieriXBarbosa   936990437100879873   \n",
       "4                     NaN       Borges2510           1712396508   \n",
       "..                    ...              ...                  ...   \n",
       "614                   NaN          Tioito1   982506039055716352   \n",
       "615                   NaN        jonh__fox            896205547   \n",
       "616                   NaN      ppdegalicia             13494262   \n",
       "617                   NaN        mavitrejo   980262593515356160   \n",
       "618                   NaN      Oliveira27M           2845897565   \n",
       "\n",
       "                                             tweet_url  \n",
       "0    https://twitter.com/TaconThiago/status/1084480...  \n",
       "1    https://twitter.com/Brunobr18373270/status/108...  \n",
       "2    https://twitter.com/Pedrodon17/status/10844796...  \n",
       "3    https://twitter.com/RanieriXBarbosa/status/108...  \n",
       "4    https://twitter.com/Borges2510/status/10844825...  \n",
       "..                                                 ...  \n",
       "614  https://twitter.com/Tioito1/status/10883403423...  \n",
       "615  https://twitter.com/jonh__fox/status/108834293...  \n",
       "616  https://twitter.com/ppdegalicia/status/1087630...  \n",
       "617  https://twitter.com/mavitrejo/status/108835316...  \n",
       "618  https://twitter.com/Oliveira27M/status/1087963...  \n",
       "\n",
       "[619 rows x 39 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c21782-713b-46dd-a8cb-c7a4bf590ae7",
   "metadata": {},
   "source": [
    "#### @mentioned tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932b4c1c-d6b8-49f9-b88c-678c4ee4b61c",
   "metadata": {},
   "source": [
    "A few users @mention copies of tweets towards other specific users creating multiple copies of the same tweet - those duplicates should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b6c3bb-2b2e-4086-bf4b-947e95796a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'no_mention' containing the contents of the column 'text' without any preceding @mentions\n",
    "df_tweets_raw_data['no_mention'] = df_tweets_raw_data['text'].str.replace(r'@\\w+\\s*', '', regex=True)\n",
    "\n",
    "# Drop duplicate rows except the first occurrence based on 'no_mention'\n",
    "df_tweets_raw_data.drop_duplicates(subset='no_mention', keep='first', inplace=True)\n",
    "df_tweets_raw_data = df_tweets_raw_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0db3847-c807-4504-9324-b114074300f8",
   "metadata": {},
   "source": [
    "## Sampling the raw data according to filtering expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e818d364-65b5-40a1-aca8-30992032d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the filtering expressions\n",
    "#filter_words = ['arma', 'pátria', 'ladrão', 'cristão', 'comunista', 'família', 'liberdade', 'conservador', 'deus']\n",
    "filter_words = ['venezuela']\n",
    "\n",
    "# Creating a boolean mask for filtering\n",
    "mask = df_tweets_raw_data['text'].str.contains('|'.join(filter_words), case=False)\n",
    "\n",
    "# Applying the mask to create 'df_tweets_filtered'\n",
    "df_tweets_filtered = df_tweets_raw_data[mask]\n",
    "df_tweets_filtered = df_tweets_filtered.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db4718-37a5-4895-8f3c-cac12e4aa6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b9f3a-5604-4efa-82a5-780b9b1f7837",
   "metadata": {},
   "source": [
    "### Exporting the filtered data into a file for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e0e64-6977-47d0-a15a-9ea1abce8951",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered.to_csv('tweets_emojified.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d8c1a-2600-4b18-a1c7-0141fdcd40ea",
   "metadata": {},
   "source": [
    "## Replacing emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3478255c-a7a8-48f0-9608-999dc92464a4",
   "metadata": {},
   "source": [
    "### Demojifying the column `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341baec-2fa0-4ee8-82cf-1522257ab02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to demojify a string\n",
    "def demojify_line(input_line):\n",
    "    demojified_line = demoji.replace_with_desc(input_line, sep='<em>')\n",
    "    return demojified_line\n",
    "\n",
    "df_tweets_filtered['text'] = df_tweets_filtered['text'].apply(demojify_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08405272-4072-4cbe-92c0-c221d3848181",
   "metadata": {},
   "source": [
    "#### Exporting the filtered data into a file for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a2453a-9a6a-4811-ad67-4a7ad495d95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered.to_csv('tweets_demojified1.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820ee493-3e57-4d46-817d-b7a4af0b0722",
   "metadata": {},
   "source": [
    "### Separating the demojified strings with spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d55d111-b676-4569-89a0-a1c4ec2e57be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to separate the demojified strings with spaces\n",
    "def preprocess_line(input_line):\n",
    "    # Add a space before the first delimiter '<em>', if it is not already preceded by one\n",
    "    preprocessed_line = re.sub(r'(?<! )<em>', ' <em>', input_line)\n",
    "    # Add a space after the first delimiter '<em>', if it is not already followed by one\n",
    "    preprocessed_line = re.sub(r'<em>(?! )', '<em> ', preprocessed_line)\n",
    "    return preprocessed_line\n",
    "\n",
    "# Separating the demojified strings with spaces\n",
    "df_tweets_filtered['text'] = df_tweets_filtered['text'].apply(preprocess_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb282a-30a2-4401-80ac-305aa5f9d7c5",
   "metadata": {},
   "source": [
    "#### Exporting the filtered data into a file for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3fce08-90f6-4e6a-97f7-ae69517a3689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered.to_csv('tweets_demojified2.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b2004-f565-44f9-9d43-31bf393170d8",
   "metadata": {},
   "source": [
    "### Formatting the demojified strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2357ac57-bf81-4377-9a21-90fb79d1fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to format the demojified string\n",
    "def format_demojified_string(input_line):\n",
    "    # Defining a function to format the demojified string using RegEx\n",
    "    def process_demojified_string(s):\n",
    "            # Lowercase the string\n",
    "            s = s.lower()\n",
    "            # Replace spaces and colons followed by a space with underscores\n",
    "            s = re.sub(r'(: )| ', '_', s)\n",
    "            # Add the appropriate prefixes and suffixes\n",
    "            s = f'EMOJI{s}e'\n",
    "            return s\n",
    "\n",
    "    # Use RegEx to find and process each demojified string\n",
    "    processed_line = re.sub(r'<em>(.*?)<em>', lambda match: process_demojified_string(match.group(1)), input_line)\n",
    "    return processed_line\n",
    "\n",
    "# Formatting the demojified strings\n",
    "df_tweets_filtered['text'] = df_tweets_filtered['text'].apply(format_demojified_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e743b0db-02a0-4e0b-b0b5-d6659960bc30",
   "metadata": {},
   "source": [
    "### Replacing the `pipe` character by the `-` character in the `text` column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc943030-7448-4d4d-98b8-b03c40e113e2",
   "metadata": {},
   "source": [
    "Further on, a few columns of the dataframe are going to be exported into the file `tweets.txt` whose columns need to be delimited by the `pipe` character. Therefore, it is recommended that any occurrences of the `pipe` character in the `text` column are replaced by another character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1c3584-ae80-41fc-b03a-4286a564a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to replace the 'pipe' character by the '-' character\n",
    "def replace_pipe_with_hyphen(input_string):\n",
    "    modified_string = re.sub(r'\\|', '-', input_string)\n",
    "    return modified_string\n",
    "\n",
    "# Replacing the 'pipe' character by the '-' character\n",
    "df_tweets_filtered['text'] = df_tweets_filtered['text'].apply(replace_pipe_with_hyphen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3d28f3-7d63-4c02-ba55-0dbc957a4291",
   "metadata": {},
   "source": [
    "#### Exporting the filtered data into a file for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab47cc6-5888-418a-8e4f-97daaf2ac3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered.to_csv('tweets_demojified3.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d4ca4-2999-47d3-ae97-b4f3a7599d84",
   "metadata": {},
   "source": [
    "## Tokenising"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f950bc7-9ffa-4d96-9ce0-439643f90a55",
   "metadata": {},
   "source": [
    "Please refer to [What is tokenization in NLP?](https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d393e-e85d-48f8-93f2-d7dfec6a2062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to tokenise a string\n",
    "def tokenise_string(input_line):\n",
    "    # Replace URLs with placeholders\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\\b'\n",
    "    placeholder = '<URL>'  # Choose a unique placeholder\n",
    "    urls = re.findall(url_pattern, input_line)\n",
    "    tokenised_line = re.sub(url_pattern, placeholder, input_line)  # Replace URLs with placeholders\n",
    "    \n",
    "    # Replace curly quotes with straight ones\n",
    "    tokenised_line = tokenised_line.replace('“', '\"').replace('”', '\"').replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "    # Separate common punctuation marks with spaces\n",
    "    tokenised_line = re.sub(r'([.\\!?,\"\\'/()])', r' \\1 ', tokenised_line)\n",
    "    # Add a space before '#'\n",
    "    tokenised_line = re.sub(r'(?<!\\s)#', r' #', tokenised_line)  # Add a space before '#' if it is not already preceded by one\n",
    "    # Reduce extra spaces by a single space\n",
    "    tokenised_line = re.sub(r'\\s+', ' ', tokenised_line)\n",
    "    \n",
    "    # Replace the placeholders with the respective URLs\n",
    "    for url in urls:\n",
    "        tokenised_line = tokenised_line.replace(placeholder, url, 1)\n",
    "    \n",
    "    return tokenised_line\n",
    "\n",
    "# Tokenising the strings\n",
    "df_tweets_filtered['text'] = df_tweets_filtered['text'].apply(tokenise_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edad8cc4-bd36-4d7a-905e-55173081cd85",
   "metadata": {},
   "source": [
    "## Creating the files `file_index.txt` and `tweets.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a908ea-85cd-4ded-b752-2ce8ccee6f62",
   "metadata": {},
   "source": [
    "### Creating column `text_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b8a02-8d66-4227-838a-020f8e458225",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered['text_id'] = 't' + df_tweets_filtered.index.astype(str).str.zfill(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ecbc7-802a-4546-b770-bd94372e58f7",
   "metadata": {},
   "source": [
    "### Creating column `conversation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad070c7-3caf-4eb8-8dfe-eba469fe0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered['conversation'] = 'v:' + df_tweets_filtered['author_id'].str.replace('id_', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85be5b3-f3ec-4061-a9c7-6d29591a8efd",
   "metadata": {},
   "source": [
    "### Creating column `date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30acc9-8d7d-41ef-9e33-2367d4c1462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'created_at' to datetime format\n",
    "df_tweets_filtered['created_at'] = pd.to_datetime(df_tweets_filtered['created_at'])\n",
    "\n",
    "# Extract the date part (without time) into a new column 'date'\n",
    "df_tweets_filtered['date'] = df_tweets_filtered['created_at'].dt.date\n",
    "\n",
    "# Add the prefix 'd:' to the 'date' values\n",
    "df_tweets_filtered['date'] = 'd:' + df_tweets_filtered['date'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1067eec7-aedb-4bba-8241-5d5d000f6226",
   "metadata": {},
   "source": [
    "### Creating column `text_url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5dec9e-0791-4b40-b59e-b46b62d400ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered['text_url'] = 'url:' + df_tweets_filtered['tweet_url']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c35b3-69bc-4039-aa13-6b496f81c729",
   "metadata": {},
   "source": [
    "### Creating column `user`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700d8af-3be8-4a60-bd3d-1ddf3df356ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered['user'] = 'u:' + df_tweets_filtered['username']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1d2a83-b1a0-4db7-b9d3-e6d91202402c",
   "metadata": {},
   "source": [
    "### Creating column `content`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9313bd-6cbe-487c-a152-de678559260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered['content'] = 'c:' + df_tweets_filtered['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11872fa-2b44-4302-ab43-f98f64692eaf",
   "metadata": {},
   "source": [
    "### Reordering the created columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1538070e-1838-454f-a761-549e2bd5bc36",
   "metadata": {},
   "source": [
    "Please refer to:\n",
    "- [Python - List Comprehension 1](https://www.w3schools.com/python/python_lists_comprehension.asp)\n",
    "- [Python - List Comprehension 2](https://treyhunner.com/2015/12/python-list-comprehensions-now-in-color/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc325f4-43c7-4ba8-b10f-130974d8c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the columns (we use list comprehension to create a list of all columns except 'text_id', 'variable', 'date' and 'text_url')\n",
    "df_tweets_filtered = df_tweets_filtered[['text_id', 'conversation', 'date', 'text_url', 'user', 'content'] + [col for col in df_tweets_filtered.columns if col not in ['text_id', 'conversation', 'date', 'text_url', 'user', 'content']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1217907f-187c-4d64-a97a-e7a9c607f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f23e3-bc12-4150-8bf1-557184b40d1b",
   "metadata": {},
   "source": [
    "### Creating the file `file_index.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199bbbe5-a366-4fde-9314-f0de3d49c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered[['text_id', 'conversation', 'date', 'text_url']].to_csv('file_index.txt', sep=' ', index=False, header=False, encoding='utf-8', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd415c21-39dc-4083-9632-e5cd92fa6948",
   "metadata": {},
   "source": [
    "### Creating the file `tweets.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5445fc96-0b9f-4283-80b3-09b1bddeab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'tweets'\n",
    "try:\n",
    "    os.mkdir(folder)\n",
    "    print(f'Folder {folder} created!')\n",
    "except FileExistsError:\n",
    "    print(f'Folder {folder} already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de49e26-5efe-472f-9952-cef41fcda082",
   "metadata": {},
   "source": [
    "Note: The parameters `doublequote=False` and `escapechar=' '` are required to avoid that the column content is doublequoted with '\"' in sentences that use characters that need to be escaped such as double quote '\"' itself - this causes a malformed response from TreeTagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b566e03-56e4-44a8-8558-b6e940bb6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_filtered[['text_id', 'conversation', 'date', 'user', 'content']].to_csv(f'{folder}/tweets.txt', sep='|', index=False, header=False, encoding='utf-8', lineterminator='\\n', doublequote=False, escapechar=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f769843-4ff9-4436-a2e0-d61e2c391e9d",
   "metadata": {},
   "source": [
    "## Tagging with TreeTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3877be9-6317-42ee-8e8d-c15c48f77996",
   "metadata": {},
   "source": [
    "- On Visual Studio Code (VS Code), open the folder where your project is located with `Open Folder...`\n",
    "- Open a WSL Ubuntu Terminal on VS Code\n",
    "- **Important**: Activate the `my_env` Python environment by executing `source \"$HOME\"/my_env/bin/activate`\n",
    "- Proceed as indicated\n",
    "\n",
    "Note: You have to download and open this Jupyter Notebook on JupyterLab (provided as part of Anaconda Distribution) to visualise the procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720724ab-5e77-4e9e-b05a-24cf73259cd0",
   "metadata": {},
   "source": [
    "Purpose: Annotate the texts in `tweets/tweets.txt` with part-of-speech and lemma information.\n",
    "- Input\n",
    "    - `file_index.txt`\n",
    "    - `tweets/tweets.txt`\n",
    "- Output\n",
    "    - `tweets/tagged.txt`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9005cc09-f5ef-454f-9b74-4d7ca9fe4f10",
   "metadata": {},
   "source": [
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ source \"$HOME\"/my_env/bin/activate\n",
    "(my_env) eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ sh treetagging.sh\n",
    "--- treetagging t000000 / t018205 ---\n",
    "        reading parameters ...\n",
    "        tagging ...\n",
    "         finished.\n",
    "--- treetagging t000001 / t018205 ---\n",
    "        reading parameters ...\n",
    "        tagging ...\n",
    "         finished.\n",
    "--- treetagging t000002 / t018205 ---\n",
    "        reading parameters ...\n",
    "        tagging ...\n",
    "         finished.\n",
    "--- treetagging t000003 / t018205 ---\n",
    "        reading parameters ...\n",
    "        tagging ...\n",
    "         finished.\n",
    "<omitted>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0060ba-d01c-4e48-a353-8c9ba0fc456e",
   "metadata": {},
   "source": [
    "## Processing `tokenstypes`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93351bf2-3457-4b47-8a47-7ff13c4e0e1a",
   "metadata": {},
   "source": [
    "Purpose: Capture the content tokens (specific occurrences of words) and the content types (general concept of words) from `tweets/tagged.txt`.\n",
    "- Input\n",
    "    - `file_index.txt`\n",
    "    - `tweets/tagged.txt`\n",
    "- Output\n",
    "    - `tweets/tokens.txt`\n",
    "    - `tweets/types.txt`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6cddc717-8ff2-4f38-ae10-8d6616de6a6c",
   "metadata": {},
   "source": [
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ source \"$HOME\"/my_env/bin/activate\n",
    "(my_env) eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ sh tokenstypes.sh\n",
    "--- tokenstypes t000000 / 18206 ---\n",
    "--- tokenstypes t000001 / 18206 ---\n",
    "--- tokenstypes t000002 / 18206 ---\n",
    "--- tokenstypes t000003 / 18206 ---\n",
    "--- tokenstypes t000004 / 18206 ---\n",
    "--- tokenstypes t000005 / 18206 ---\n",
    "<omitted>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef598329-fb5a-4b07-8799-b2bbb55847f1",
   "metadata": {},
   "source": [
    "## Processing `toplemmas`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce5796-0434-44d5-88e2-8f934a6cebc8",
   "metadata": {},
   "source": [
    "Purpose: Determine the 1.000 top lemmas. **Important**: This process requires manual inspection. Non-meaningful lemmas should be excluded by updating `stoplist.sed` and reiterating the processing.\n",
    "- Input\n",
    "    - `tweets/types.txt`\n",
    "    - `stoplist.sed`: List of rules that allows the exclusion of a certain lemmas\n",
    "- Output\n",
    "    - `selectedwords` = `var_index.txt`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "13b0fc8e-73e2-4e2d-9e2f-73068e4132fc",
   "metadata": {},
   "source": [
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ source \"$HOME\"/my_env/bin/activate\n",
    "(my_env) eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ sh toplemmas.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a4675-5067-4e8b-8ca8-baf9f1b915c1",
   "metadata": {},
   "source": [
    "## Processing `sas`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d158b2-73d7-477e-a7a7-2b87216a19ae",
   "metadata": {},
   "source": [
    "Purpose: Prepare input data for processing in SAS.\n",
    "- Input\n",
    "    - `tweets/types.txt`\n",
    "    - `selectedwords`\n",
    "    - `file_index.txt`\n",
    "- Output\n",
    "    - `columns`\n",
    "    - `sas/data.txt`\n",
    "    - `sas/dates.txt`\n",
    "    - `sas/wcount.txt`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d250d29-861c-41a6-9793-e0df758100e0",
   "metadata": {},
   "source": [
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ source \"$HOME\"/my_env/bin/activate\n",
    "(my_env) eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ sh sas.sh\n",
    "--- v000001 ---\n",
    "--- v000002 ---\n",
    "--- v000003 ---\n",
    "--- v000004 ---\n",
    "--- v000005 ---\n",
    "<omitted>\n",
    "--- v001000 ---\n",
    "[nltk_data] Downloading package punkt to /home/eyamrog/nltk_data...\n",
    "[nltk_data]   Package punkt is already up-to-date!\n",
    "Word counts written to sas/wcount.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8087ea-ffd4-43b9-8eda-4444b931c319",
   "metadata": {},
   "source": [
    "## Processing `datamatrix`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4374f48-04b6-48af-a31d-125ef0e85972",
   "metadata": {},
   "source": [
    "Purpose: Prepares input data for calculating the correlation matrix.\n",
    "- Input\n",
    "    - `file_index.txt`\n",
    "    - `columns`\n",
    "    - `selectedwords`\n",
    "- Output\n",
    "    - `file_ids.txt`\n",
    "    - `data.csv`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23925f09-05ef-44a5-ba48-78cac1dedc76",
   "metadata": {},
   "source": [
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ source \"$HOME\"/my_env/bin/activate\n",
    "(my_env) eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ sh datamatrix.sh\n",
    "(my_env) eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ sh datamatrix.sh\n",
    "--- v000001 ---\n",
    "--- v000002 ---\n",
    "--- v000003 ---\n",
    "--- v000004 ---\n",
    "--- v000005 ---\n",
    "<omitted>\n",
    "--- v001000 ---\n",
    "--- data.csv ...---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6130e8f0-a7e5-4f6e-b427-090f7cda4856",
   "metadata": {},
   "source": [
    "## Processing `correlationmatrix`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bddbb87-b4de-4c41-8177-b2f677d803b6",
   "metadata": {},
   "source": [
    "Purpose: Calculates the correlation matrix.\n",
    "- Input\n",
    "    - `data.csv`\n",
    "- Output\n",
    "    - `correlation`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42927846-5d87-4f5f-8ce6-8beb4ef733a4",
   "metadata": {},
   "source": [
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ source \"$HOME\"/my_env/bin/activate\n",
    "(my_env) eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ sh correlationmatrix.sh\n",
    "--- python correlation ... ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9203f-e9e1-4a73-8bed-7dec1ad8617e",
   "metadata": {},
   "source": [
    "## Processing `formats`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69517100-c28d-44ba-90a7-37988e3df8ad",
   "metadata": {},
   "source": [
    "Purpose: Prepare input data for processing in SAS.\n",
    "- Input\n",
    "    - `data.csv`\n",
    "    - `selectedwords`\n",
    "- Output\n",
    "    - `sas/corr.txt`\n",
    "    - `sas/word_labels_format.sas`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96f9b02f-fd33-4ff2-bf57-463fac9743e6",
   "metadata": {},
   "source": [
    "eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ source \"$HOME\"/my_env/bin/activate\n",
    "(my_env) eyamrog@Rog-ASUS:/mnt/c/Users/eyamr/Downloads$ sh formats.sh\n",
    "--- sas/sas/corr.txt ---\n",
    "--- sas/word_labels_format.sas ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd38617b-0487-4ba2-b619-d9b78aba9f2c",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf501e4a-1193-4999-bdc0-88c08d2840f7",
   "metadata": {},
   "source": [
    "Right-click on the link and choose `Save link as` to download the corresponding file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73ba45-9d6c-4578-b14e-c80c3ccaafe4",
   "metadata": {},
   "source": [
    "- [CL_St1_Querem_Results.zip](https://laelgelcquerem.s3.sa-east-1.amazonaws.com/CL_St1_Querem_Results.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb169464-4f29-4513-b272-46f201deff17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
